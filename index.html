<!DOCTYPE html>
<html>
    <head>
        <meta charset="UTF-8">
        <meta name="description" content="This my homepage">
        <!-- setting for optimal display wth size of the screen -->
        <meta name="<meta name="viewport" content="width=device-width, initial-scale=1">

        <title>Intuitive Facial Animation Editing based on a Generative RNN Framework </title>
        <link rel="stylesheet" href="_css/style.css" type="text/css">
        <!-- internal style 
    <style type="text/css">
    </style
    -->
    </head>

    <body>
        <!-- color w"schoolcolor -->
        <header>
        <!-- <nav></nav> navigation menu
        -->
        </header>

        <main >
        <!-- <article></article> article 
             <section></section> section
        -->
        <div class="paper">
            <h1 class="title">Intuitive Facial Animation Editing based on a Generative RNN Framework</h1>
            <br/>  
            <div class="teaser">
                <img class="fig-teaser" src="./data/front_page.png" width="800" alt="Teaser">
            </div>
            <!-- alt what it's show up if no figure is found -->
            <div class="section" id="abstract">
                <h3 class="subtitle">Abstract</h3>
                    <p> For the last decades, the concern of producing convincing facial animation has garnered great interest, that has only been accelerating with the recent explosion of 3D content in both entertainment and professional activities. The use of motion capture and retargeting has arguably become the dominant solution to address this demand. Yet, despite high level of quality and automation performance-based animation pipelines still require manual cleaning and editing to refine raw results, which is a time- and skill-demanding process. In this paper, we look to leverage machine learning to make facial animation editing faster and more accessible to non-experts. Inspired by recent <i>image inpainting</i> methods, we design a generative recurrent neural network that generates realistic motion into designated segments of an existing facial animation, optionally following user-provided guiding constraints. Our system handles different supervised or unsupervised editing scenarios such as motion filling during occlusions, expression corrections, semantic content modifications, and noise filtering. We demonstrate the usability of our system on several animation editing use cases.</p>
            </div>
                <!-- br/ = break  hr/ = ligne -->
            <div class="section" id="download">
                <h3 class="subtitle">Download</h3>
                <div class="first-page">
                <a class="first paper page"> <!-- TODO href="paperi_link.pdf" -->
                    <img class="fig-first-page" alt="paper (??MB)" src="../data/generative_rnn.jpg">
                </a>
                </div>
                <span>
                [<a alt="paper" href="data/paper.pdf" target="_blank">pdf</a>]
                [<a alt="video" href="publications/sca-2019/video.mp4" target="_blank">video</a>]
                [<a alt="bibtex" href="publications/sca-2020/bibtex.bib" target="_blank">bibtex</a>]
                </span>
            </div>
            <br/>
            <div class="section" id="data">
                <h3 class="subtitle">Dataset</h3>
                    <p> <!--- style=i"color; background-color = grey" --> 
                    The database has been created using the 
                    <a href="https://data.vision.ee.ethz.ch/cvl/datasets/b3dac2.en.html" target="_blank"> 3-D Audio-Visual Corpus of Affective Communication</a> and can be obtained upon request, for research purposes only.
                    <!-- target where the page willopen -->
                    To obtain the data, please send a mail at <b>research</b> (at) <b>dynamixyz.com</b> providing your name, your organization and your research interest.
                    </p>
            </div>
            <br/>

            <div class="section" id="reference">
                <h3 class="subtitle">Referencing the database in your work</h3>
                <ul> 
                    <li>- E. Berson, C. Soladi√©, N. Stoiber, "Intuitive Facial Animation Editing based on a Generative RNN Framework"</li> 
                    <br/>
                    <li>-  G. Fanelli, J. Gall, H. Romsdorfer, T.Weise, L. Van Gool, "A 3-D Audio-Visual Corpus of Affective Communication", IEEE Transactions on Multimedia, Vol. 12, No. 6, pp. 591 - 598, 2010.</li>
                </ul>
            </div>
        </div>
    </body>
</html>
